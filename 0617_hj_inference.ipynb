{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7c8e06",
   "metadata": {},
   "source": [
    "# Set Path & Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5c9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = './models/RegNet_0.001_Lamb_CosineAnnealing_example.pth' # *\n",
    "# model_path = './models/RegNet_0.001_adam_CosineAnnealing_example.pth'\n",
    "# model_path = './models/RegNet_1e-05_adam_CosineAnnealing_example.pth'\n",
    "# model_path = './models/RegNet_1e-05_Lamb_CosineAnnealing_example.pth'\n",
    "model_path = './models/RegNet_1e-05_rmsprop_CosineAnnealing_example.pth' # *\n",
    "# model_path = './models/RegNet_1e-05_nadam_CosineAnnealing_example.pth'\n",
    "# model_path = './models/ResNet50_0.001_adam_CosineAnnealing_example.pth'\n",
    "# model_path = './models/ResNet50_0.001_Lamb_CosineAnnealing_example.pth'\n",
    "# model_path = './models/ResNet50_0.001_rmsprop_CosineAnnealing_example.pth'\n",
    "# model_path = './models/ResNet50_1e-05_Lamb_CosineAnnealing_example.pth'\n",
    "# model_path = './models/ResNet50_1e-05_adam_CosineAnnealing_example.pth'\n",
    "# model_path = './models/EfficientNetb4_0.001_adam_CosineAnnealing_example.pth'\n",
    "# model_path = './models/EfficientNetb4_0.001_Lamb_CosineAnnealing_example.pth'\n",
    "# model_path = './models/EfficientNetb4_1e-05_rmsprop_CosineAnnealing_example.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d47ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'D:/jupyter_home/3_project/test_img/test_all/' \n",
    "path = 'D:/jupyter_home/3_project/test_img/test_one/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d4d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_model = 'resnet'\n",
    "# used_model = 'efficientnet'\n",
    "used_model = 'regnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63c13",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac329bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from glob import glob\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c600b",
   "metadata": {},
   "source": [
    "# Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddad2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        total_images_path = glob(path + '*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "            file_names.sort()\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.test_file_name = file_names\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size : {len(self.test_file_name)}')\n",
    "        print(self.test_file_name)\n",
    "\n",
    "    def __getitem__(self, idx): # test 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "        image = cv2.imread(opj(path, self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa36578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_augmentation(img_size):\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "                ])\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dd3ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size : 1\n",
      "['55702_1.jpg']\n"
     ]
    }
   ],
   "source": [
    "test_transform = get_test_augmentation(img_size=256)\n",
    "test_dataset = Test_dataset(path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc8726",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':50,\n",
    "    'PATIENCE':10,\n",
    "    'class':14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacb5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(2048, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,CFG['class'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # x = x.mean(dim=(-2, -1))\n",
    "        # (batch, 2048, 4, 4)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1263d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetb4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetb4, self).__init__()\n",
    "        model = models.efficientnet_b4(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(1792, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 1792, 1, 1)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a09b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegNet, self).__init__()\n",
    "        model = models.regnet_y_16gf(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(3024, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 3024, 1, 1)\n",
    "        \n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f010d3",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f9195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    if encoder_name == 'resnet':\n",
    "        model = ResNet50().to(device)\n",
    "    elif encoder_name == 'efficientnet':\n",
    "        model = EfficientNetb4().to(device)\n",
    "    elif encoder_name == 'regnet':\n",
    "        model = RegNet().to(device)\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1b48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9664caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "predict_arr = predict(used_model, test_loader, device, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab29a3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.53626781e-04, 1.74887409e-05, 3.37045931e-04, 1.27747902e-04,\n",
       "        2.58083828e-03, 3.45359440e-04, 5.63827343e-03, 7.01616409e-06,\n",
       "        2.11710273e-03, 1.11962894e-04, 5.38883323e-04, 1.89988015e-04,\n",
       "        1.13925757e-03, 9.86695409e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9a9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175b62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['25222_대만)망고케익184g', '25228_대만)파인애플케익184G', '35211_매일유업)데르뜨130G', '35584_매일데르뜨파인애플90G', '35585_매일데르뜨감귤90G', '45030_돌황도666G', '45657_씨제이)쁘티첼(요거젤리복숭아)', '45658_씨제이)쁘티첼(요거젤리밀감)', '45659_씨제이)쁘티첼(요거젤리딸기)', '45660_씨제이)쁘티첼(요거젤리화이트코코)', '45661_씨제이)쁘티첼(요거젤리블루베리)', '55034_돌트로피칼666G', '55701_쁘띠첼요거젤리밀감', '55702_쁘띠첼요거젤리복숭아']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754ee639",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {string : i for i, string in enumerate(labels)}\n",
    "label_decoder = {val:key for key, val in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cb31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55702_쁘띠첼요거젤리복숭아\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "\n",
    "for i in range(len(test_dataset.test_file_name)):\n",
    "    prediction = label_decoder[predict_arr.argmax(axis=1)[i]]\n",
    "    print(prediction)\n",
    "    pred_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e323b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471c185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439a5567",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3ced8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# import torch.nn.functional as F\n",
    "# import albumentations as A\n",
    "# import albumentations.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d060fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_valid_data(data_dir):\n",
    "#     img_valid_list = []\n",
    "#     label_valid_list = []\n",
    "    \n",
    "#     image_path = os.path.join(data_dir, 'dessert')\n",
    "    \n",
    "#     for product_name in os.listdir(image_path):\n",
    "#         product_path = os.path.join(image_path, product_name)\n",
    "#         if os.path.isdir(product_path):\n",
    "#             # get image path\n",
    "#             img_valid_list.extend(glob(os.path.join(product_path, '*.jpg')))\n",
    "#             img_valid_list.extend(glob(os.path.join(product_path, '*.png')))\n",
    "#             label = list(product_name[:5])\n",
    "            \n",
    "#             # get label\n",
    "#             label_valid_list.append(''.join(label))\n",
    "                \n",
    "#     return img_valid_list, label_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45d3d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valid_data_blanced(img, label):\n",
    "#     x = []\n",
    "#     y = []\n",
    "    \n",
    "#     for i in range(CFG['class']):\n",
    "#         _img = img[(i * 15): ((i + 1) * 15)]\n",
    "#         _label = label[i]\n",
    "        \n",
    "#         for img_product in _img:\n",
    "#             x.append(img_product)\n",
    "#             y.append(_label)\n",
    "            \n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "973a8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_valid_list, label_valid_list = get_valid_data('./Data/product_image/Validation/')\n",
    "# x_valid, y_valid = valid_data_blanced(img_valid_list, label_valid_list)\n",
    "# len(label_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b00b0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le2 = preprocessing.LabelEncoder()\n",
    "# targets_y = le2.fit_transform(y_valid)\n",
    "# targets_y = torch.as_tensor(targets_y)\n",
    "# one_hot_valid_y = F.one_hot(targets_y)\n",
    "# one_hot_valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e85ca4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlbumentationsCustomDataset(Dataset):\n",
    "#     def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "#         self.transforms = transforms\n",
    "#         self.train_mode = train_mode\n",
    "#         self.img_path_list = img_path_list\n",
    "#         self.label_list = label_list\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img_path = self.img_path_list[index]\n",
    "#         # Get image data\n",
    "#         image = cv2.imread(img_path)\n",
    "        \n",
    "#         # By default OpenCV uses BGR color space for color images,\n",
    "#         # so we need to convert the image to RGB color space.\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         if self.train_mode:\n",
    "# #             image = image.astype(np.int16)\n",
    "#             augmented = self.transforms(image=image)\n",
    "#             image = augmented['image']\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "#         else:\n",
    "#             image = self.transforms(image)\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8305a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_test_transform = albumentations.Compose([\n",
    "#                                     A.Resize(256, 256),\n",
    "#                                     A.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "# #                                     A.pytorch.transforms.ToTensor(),\n",
    "#                                     A.pytorch.transforms.ToTensorV2(transpose_mask=True),\n",
    "#                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccfea251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_vali_dataset = AlbumentationsCustomDataset(x_valid, one_hot_valid_y, train_mode=True, transforms=A_test_transform)\n",
    "# A_vali_loader = DataLoader(A_vali_dataset, batch_size = 5, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85729923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_for_acc(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     model_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for img, label in tqdm(iter(test_loader)):\n",
    "#             img = img.float().to(device)\n",
    "            \n",
    "#             pred_logit = model(img)\n",
    "#             pred_logit = pred_logit.squeeze().detach().cpu()\n",
    "            \n",
    "#             model_pred.extend(pred_logit.tolist())\n",
    "#     return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76b9b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7336bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if used_model == 'resnet':\n",
    "#     model = ResNet50().to(device)\n",
    "# elif used_model == 'efficientnet':\n",
    "#     model = EfficientNetb4().to(device)\n",
    "# elif used_model == 'regnet':\n",
    "#     model = RegNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8932b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "# preds = predict_for_acc(model, A_vali_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fdb7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = np.argmax(preds, axis=1)\n",
    "# true_labels = one_hot_valid_y.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f0b6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ad1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53f81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed596327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_16] *",
   "language": "python",
   "name": "conda-env-pytorch_16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
