{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7c8e06",
   "metadata": {},
   "source": [
    "# Set Path & Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5c9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_0.001_Lamb_CosineAnnealing_example.pth' # 0.875\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_0.001_adam_CosineAnnealing_example.pth' # 0.59375\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_adam_CosineAnnealing_example.pth' # 0.8125\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_Lamb_CosineAnnealing_example.pth' # 0.40625\n",
    "model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_rmsprop_CosineAnnealing_example.pth' # 0.90625\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_nadam_CosineAnnealing_example.pth' # 0.84375\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_0.001_adam_CosineAnnealing_example.pth'  # 0.34375\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_0.001_Lamb_CosineAnnealing_example.pth' # 0.6875\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_0.001_rmsprop_CosineAnnealing_example.pth' # 0.1875\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_1e-05_Lamb_CosineAnnealing_example.pth' # 0.125\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_1e-05_adam_CosineAnnealing_example.pth' # 0.65625\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_0.001_adam_CosineAnnealing_example.pth' # 0.71875\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_0.001_Lamb_CosineAnnealing_example.pth' # 0.6875\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_1e-05_rmsprop_CosineAnnealing_example.pth' # 0.59375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d47ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/lab16/jupyter_home/test_img/test_all/' \n",
    "# path = '/home/lab16/jupyter_home/test_img/test_one/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d4d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_model = 'resnet'\n",
    "# used_model = 'efficientnet'\n",
    "used_model = 'regnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63c13",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac329bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from glob import glob\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c600b",
   "metadata": {},
   "source": [
    "# Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddad2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        total_images_path = glob(path + '*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "            file_names.sort()\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.test_file_name = file_names\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size : {len(self.test_file_name)}')\n",
    "        print(self.test_file_name)\n",
    "\n",
    "    def __getitem__(self, idx): # test 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "        image = cv2.imread(opj(path, self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa36578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_augmentation(img_size):\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "                ])\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dd3ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size : 32\n",
      "['25222_1.jpg' '25222_2.jpg' '25222_3.jpg' '25222_4.jpg' '25228_1.jpg'\n",
      " '35211_1.jpg' '35211_2.jpg' '35211_3.jpg' '35211_4.jpg' '35584_1.jpg'\n",
      " '35584_2.jpg' '35585_1.jpg' '45030_1.jpg' '45030_2.jpg' '45657_1.jpg'\n",
      " '45657_2.jpg' '45657_3.jpg' '45658_001.jpg' '45659_1.jpg' '45659_2.jpg'\n",
      " '45660_1.jpg' '45660_2.jpg' '45661_1.jpg' '45661_2.jpg' '55034_1.jpg'\n",
      " '55701_1.jpg' '55701_2.jpg' '55701_3.jpg' '55701_4.jpg' '55702_1.jpg'\n",
      " '55702_2.jpg' '55702_3.jpg']\n"
     ]
    }
   ],
   "source": [
    "test_transform = get_test_augmentation(img_size=256)\n",
    "test_dataset = Test_dataset(path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc8726",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':50,\n",
    "    'PATIENCE':10,\n",
    "    'class':14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacb5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(2048, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,CFG['class'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # x = x.mean(dim=(-2, -1))\n",
    "        # (batch, 2048, 4, 4)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1263d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetb4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetb4, self).__init__()\n",
    "        model = models.efficientnet_b4(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(1792, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 1792, 1, 1)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a09b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegNet, self).__init__()\n",
    "        model = models.regnet_y_16gf(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(3024, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 3024, 1, 1)\n",
    "        \n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f010d3",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f9195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    if encoder_name == 'resnet':\n",
    "        model = ResNet50().to(device)\n",
    "    elif encoder_name == 'efficientnet':\n",
    "        model = EfficientNetb4().to(device)\n",
    "    elif encoder_name == 'regnet':\n",
    "        model = RegNet().to(device)\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1b48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9664caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_arr = predict(used_model, test_loader, device, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab29a3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.84108508e-01, 9.37801553e-04, 3.65923904e-03, 1.45520738e-04,\n",
       "        1.69989711e-03, 3.73077579e-04, 3.58071481e-03, 2.30638761e-04,\n",
       "        2.36189296e-03, 4.23873542e-04, 3.97251133e-04, 4.21557546e-04,\n",
       "        5.62316418e-05, 1.60380406e-03],\n",
       "       [9.72187221e-01, 9.04397340e-04, 1.68260408e-03, 5.77530125e-04,\n",
       "        3.50908632e-03, 3.26103740e-03, 7.39689125e-03, 2.38553691e-03,\n",
       "        2.08740216e-03, 4.78498987e-04, 2.78433505e-03, 1.72759523e-03,\n",
       "        1.99009170e-04, 8.18886620e-04],\n",
       "       [9.74634707e-01, 4.53849905e-04, 1.65019417e-04, 1.76237460e-04,\n",
       "        1.42624704e-04, 2.96552945e-03, 1.52107347e-02, 5.41360641e-04,\n",
       "        6.09515351e-04, 1.05836254e-03, 1.72550231e-03, 5.86631068e-04,\n",
       "        9.77844757e-05, 1.63221115e-03],\n",
       "       [4.61616963e-01, 1.13212923e-02, 2.30590557e-03, 3.73712601e-03,\n",
       "        5.41852489e-02, 3.13674621e-02, 2.58377716e-02, 1.36762857e-03,\n",
       "        8.68493225e-03, 1.15607900e-03, 3.86143416e-01, 5.94318099e-03,\n",
       "        1.04927050e-03, 5.28372731e-03],\n",
       "       [9.93606091e-01, 9.89474705e-04, 1.41166965e-03, 7.26285652e-05,\n",
       "        7.27590959e-05, 2.16770510e-04, 9.54573392e-04, 9.90535016e-04,\n",
       "        4.13473375e-04, 3.56979639e-04, 1.22689627e-04, 7.21628719e-04,\n",
       "        2.36068954e-05, 4.70586383e-05],\n",
       "       [1.00562006e-01, 3.94814386e-04, 6.85732663e-01, 7.88865495e-04,\n",
       "        3.39595805e-04, 1.96712706e-02, 1.64978090e-03, 2.72016390e-04,\n",
       "        1.51116839e-02, 1.34482188e-03, 8.16048030e-03, 1.63302496e-01,\n",
       "        1.15070515e-03, 1.51878304e-03],\n",
       "       [4.23279265e-03, 9.42874409e-04, 5.88519812e-01, 5.91461663e-04,\n",
       "        1.07823135e-02, 9.25561786e-03, 4.73418506e-03, 5.75718703e-04,\n",
       "        3.71253997e-01, 3.13406432e-04, 3.40033206e-03, 3.14924214e-03,\n",
       "        1.27475429e-03, 9.73518938e-04],\n",
       "       [5.16437413e-03, 9.95354130e-05, 9.90676999e-01, 5.13547748e-05,\n",
       "        5.19281784e-05, 1.52733875e-04, 1.46043499e-03, 1.32269182e-04,\n",
       "        1.16488338e-03, 1.06767780e-04, 5.65302398e-05, 7.30676169e-04,\n",
       "        4.28107887e-05, 1.08615051e-04],\n",
       "       [6.79397769e-03, 2.87601724e-04, 5.05370677e-01, 4.07715212e-04,\n",
       "        2.59218030e-02, 1.28727756e-03, 1.45471944e-02, 1.03738450e-03,\n",
       "        4.34535772e-01, 6.17906800e-04, 5.86392824e-03, 1.61017536e-03,\n",
       "        7.03632832e-04, 1.01490773e-03],\n",
       "       [2.95423757e-04, 1.56130421e-03, 1.25290884e-04, 9.81079459e-01,\n",
       "        1.32586877e-03, 5.03720832e-04, 7.21144606e-05, 2.75059743e-03,\n",
       "        2.56295374e-04, 6.29394024e-04, 5.24138566e-03, 1.51226751e-03,\n",
       "        3.30388010e-03, 1.34310371e-03],\n",
       "       [9.96403513e-04, 2.30810815e-03, 3.39580956e-03, 8.42455208e-01,\n",
       "        1.08543649e-01, 5.01651410e-03, 7.86696386e-04, 7.64252152e-03,\n",
       "        1.52279285e-03, 1.97912508e-04, 1.51466969e-02, 2.74521951e-03,\n",
       "        8.41339212e-03, 8.29073484e-04],\n",
       "       [2.23236950e-03, 7.21360324e-04, 8.20207468e-04, 7.95383647e-04,\n",
       "        9.67566609e-01, 3.31918336e-03, 2.74606841e-03, 6.19453087e-04,\n",
       "        3.56012350e-03, 5.69366850e-04, 2.69642146e-03, 1.71034352e-03,\n",
       "        1.75990909e-03, 1.08831888e-02],\n",
       "       [1.35891732e-05, 8.61667140e-05, 3.55673037e-05, 1.97134432e-05,\n",
       "        7.96702880e-05, 9.99465764e-01, 8.29854471e-05, 8.64828780e-06,\n",
       "        2.02525589e-05, 3.87562204e-06, 1.91189374e-05, 4.43035024e-05,\n",
       "        8.96962374e-05, 3.05802787e-05],\n",
       "       [2.56503813e-06, 3.43152647e-06, 4.38025791e-06, 8.04684873e-07,\n",
       "        2.34641948e-06, 9.99894023e-01, 5.32876311e-06, 6.64264974e-07,\n",
       "        9.61067713e-07, 5.37317248e-07, 1.97254349e-06, 7.14307826e-05,\n",
       "        9.31193699e-06, 2.22437757e-06],\n",
       "       [6.36970741e-04, 2.10241251e-05, 9.49880487e-05, 2.26501597e-05,\n",
       "        4.11882764e-04, 1.01428137e-04, 9.89315629e-01, 4.75660752e-04,\n",
       "        4.53186221e-03, 6.17883925e-05, 4.41674383e-05, 3.57468416e-05,\n",
       "        1.66118116e-05, 4.22960799e-03],\n",
       "       [1.11850568e-04, 1.54206264e-05, 4.07702319e-05, 8.33525883e-06,\n",
       "        1.75811307e-04, 7.59052346e-05, 9.96642947e-01, 3.91945563e-04,\n",
       "        1.10808352e-03, 1.54827394e-05, 2.40060835e-05, 1.03654666e-05,\n",
       "        7.90516242e-06, 1.37126958e-03],\n",
       "       [1.40415043e-01, 1.32515922e-03, 4.33051616e-01, 9.23565356e-04,\n",
       "        1.66684517e-03, 1.57412700e-03, 4.70288508e-02, 1.37118401e-03,\n",
       "        4.02499586e-02, 3.80530469e-02, 6.65237894e-03, 2.82653030e-02,\n",
       "        8.75174126e-04, 2.58547664e-01],\n",
       "       [5.27223574e-06, 3.05274916e-05, 3.46939873e-06, 1.43358520e-05,\n",
       "        2.33431547e-05, 3.44045184e-05, 2.70718579e-06, 4.06314968e-04,\n",
       "        1.18712114e-05, 3.78403893e-05, 7.88769285e-06, 1.41359960e-05,\n",
       "        9.99305129e-01, 1.02802878e-04],\n",
       "       [1.10212341e-03, 3.64410698e-05, 7.43593206e-04, 1.55934067e-05,\n",
       "        3.92480579e-04, 7.68076206e-05, 1.15812419e-03, 2.05702381e-04,\n",
       "        9.94207263e-01, 4.53670036e-05, 8.84521287e-04, 2.29880185e-04,\n",
       "        1.41569297e-04, 7.60492985e-04],\n",
       "       [6.23257598e-04, 1.65354679e-04, 2.15459894e-03, 3.47352936e-04,\n",
       "        2.93822354e-03, 6.52723771e-04, 2.42949184e-03, 9.16259130e-04,\n",
       "        9.84231472e-01, 1.44968057e-04, 1.16164738e-03, 3.23192275e-04,\n",
       "        9.75959469e-04, 2.93540885e-03],\n",
       "       [1.42161021e-04, 1.36237111e-06, 3.07957125e-05, 4.03504009e-06,\n",
       "        6.31248497e-07, 3.88481840e-06, 1.28096171e-05, 6.32890806e-06,\n",
       "        1.62580036e-05, 9.99383926e-01, 2.91947727e-05, 2.25227377e-05,\n",
       "        1.17907157e-05, 3.34294949e-04],\n",
       "       [1.58770960e-02, 5.42746042e-04, 3.22089670e-03, 1.67910627e-03,\n",
       "        8.54829326e-04, 8.47409386e-03, 7.44180288e-03, 4.24525375e-03,\n",
       "        4.48903674e-03, 9.13996696e-01, 4.64614667e-03, 4.89077857e-03,\n",
       "        5.36618568e-03, 2.42753327e-02],\n",
       "       [1.01152260e-03, 1.70354650e-03, 9.16966412e-04, 2.89269839e-03,\n",
       "        4.46304074e-03, 9.05818418e-02, 9.28504684e-04, 6.12291100e-04,\n",
       "        8.55862908e-03, 5.70557192e-02, 6.76467001e-01, 3.01283714e-03,\n",
       "        8.47200081e-02, 6.70753941e-02],\n",
       "       [3.47538735e-03, 1.22708757e-03, 1.14846742e-02, 3.60649778e-03,\n",
       "        1.91696268e-02, 2.65559256e-01, 1.45125333e-02, 8.70520156e-03,\n",
       "        1.22441659e-02, 8.85197055e-03, 6.01053476e-01, 3.10482252e-02,\n",
       "        1.38196582e-02, 5.24231838e-03],\n",
       "       [9.40437603e-04, 4.04019374e-04, 3.01419408e-04, 2.56096682e-04,\n",
       "        7.13662448e-05, 1.90803390e-02, 7.79536349e-05, 3.40122904e-04,\n",
       "        1.88438426e-04, 2.79944128e-04, 8.50160490e-04, 9.76650655e-01,\n",
       "        4.17150353e-04, 1.41872020e-04],\n",
       "       [2.15199267e-04, 3.02412373e-04, 8.56939179e-04, 1.50651656e-04,\n",
       "        1.53218387e-02, 4.34529828e-03, 8.92505457e-04, 9.50384792e-03,\n",
       "        4.43100557e-03, 1.67055448e-04, 4.53613378e-04, 6.46862143e-04,\n",
       "        9.62373912e-01, 3.38956248e-04],\n",
       "       [1.72725285e-03, 7.87425553e-04, 1.74939894e-04, 4.68284969e-04,\n",
       "        2.58417713e-04, 3.13715922e-04, 9.72114431e-05, 3.18160467e-02,\n",
       "        1.14204676e-03, 8.46772164e-04, 1.13305601e-03, 2.66034179e-03,\n",
       "        9.51690793e-01, 6.88370643e-03],\n",
       "       [5.27223574e-06, 3.05274916e-05, 3.46939873e-06, 1.43358520e-05,\n",
       "        2.33431547e-05, 3.44045184e-05, 2.70718579e-06, 4.06314968e-04,\n",
       "        1.18712114e-05, 3.78403893e-05, 7.88769285e-06, 1.41359960e-05,\n",
       "        9.99305129e-01, 1.02802878e-04],\n",
       "       [5.55559791e-06, 1.28311949e-05, 1.06142988e-05, 2.01237599e-05,\n",
       "        1.31673558e-04, 8.43016969e-05, 3.76670255e-06, 2.34872321e-04,\n",
       "        3.46906490e-05, 8.33595641e-06, 1.45477215e-05, 3.90394707e-05,\n",
       "        9.99088407e-01, 3.11117270e-04],\n",
       "       [1.53626621e-04, 1.74887409e-05, 3.37045931e-04, 1.27747902e-04,\n",
       "        2.58084061e-03, 3.45359440e-04, 5.63827576e-03, 7.01615727e-06,\n",
       "        2.11710273e-03, 1.11962800e-04, 5.38883556e-04, 1.89988001e-04,\n",
       "        1.13925582e-03, 9.86695409e-01],\n",
       "       [1.70943349e-05, 1.64861945e-06, 2.73160185e-05, 5.82676967e-06,\n",
       "        1.64349243e-04, 2.60918387e-05, 3.48393689e-03, 2.11298834e-06,\n",
       "        3.90564965e-04, 1.56604710e-05, 7.97850316e-06, 1.49246971e-05,\n",
       "        8.95229517e-04, 9.94947255e-01],\n",
       "       [4.29806561e-04, 1.69238974e-05, 1.24235114e-04, 3.24531684e-05,\n",
       "        1.42873498e-04, 4.43802710e-05, 5.36455922e-02, 5.03489864e-05,\n",
       "        5.83522604e-04, 4.25613625e-03, 7.92606224e-05, 1.05001171e-04,\n",
       "        5.71082055e-04, 9.39918458e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9a9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  2,  2,  2,  2,  3,  3,  4,  5,  5,  6,  6,  2,\n",
       "       12,  8,  8,  9,  9, 10, 10, 11, 12, 12, 12, 12, 13, 13, 13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175b62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['25222_대만)망고케익184g', '25228_대만)파인애플케익184G', '35211_매일유업)데르뜨130G', '35584_매일데르뜨파인애플90G', '35585_매일데르뜨감귤90G', '45030_돌황도666G', '45657_씨제이)쁘티첼(요거젤리복숭아)', '45658_씨제이)쁘티첼(요거젤리밀감)', '45659_씨제이)쁘티첼(요거젤리딸기)', '45660_씨제이)쁘티첼(요거젤리화이트코코)', '45661_씨제이)쁘티첼(요거젤리블루베리)', '55034_돌트로피칼666G', '55701_쁘띠첼요거젤리밀감', '55702_쁘띠첼요거젤리복숭아']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754ee639",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {string : i for i, string in enumerate(labels)}\n",
    "label_decoder = {val:key for key, val in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cb31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25222_대만)망고케익184g\n",
      "25222_대만)망고케익184g\n",
      "25222_대만)망고케익184g\n",
      "25222_대만)망고케익184g\n",
      "25222_대만)망고케익184g\n",
      "35211_매일유업)데르뜨130G\n",
      "35211_매일유업)데르뜨130G\n",
      "35211_매일유업)데르뜨130G\n",
      "35211_매일유업)데르뜨130G\n",
      "35584_매일데르뜨파인애플90G\n",
      "35584_매일데르뜨파인애플90G\n",
      "35585_매일데르뜨감귤90G\n",
      "45030_돌황도666G\n",
      "45030_돌황도666G\n",
      "45657_씨제이)쁘티첼(요거젤리복숭아)\n",
      "45657_씨제이)쁘티첼(요거젤리복숭아)\n",
      "35211_매일유업)데르뜨130G\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "45659_씨제이)쁘티첼(요거젤리딸기)\n",
      "45659_씨제이)쁘티첼(요거젤리딸기)\n",
      "45660_씨제이)쁘티첼(요거젤리화이트코코)\n",
      "45660_씨제이)쁘티첼(요거젤리화이트코코)\n",
      "45661_씨제이)쁘티첼(요거젤리블루베리)\n",
      "45661_씨제이)쁘티첼(요거젤리블루베리)\n",
      "55034_돌트로피칼666G\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "55702_쁘띠첼요거젤리복숭아\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "\n",
    "for i in range(len(test_dataset.test_file_name)):\n",
    "    prediction = label_decoder[predict_arr.argmax(axis=1)[i]]\n",
    "    print(prediction)\n",
    "    pred_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e323b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471c185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439a5567",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39c1ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cbf963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images_path = glob(path + '*.jpg')\n",
    "file_names = []\n",
    "for i in range(len(total_images_path)):\n",
    "    file_names.append(os.path.basename(total_images_path[i])[:5])\n",
    "    file_names.sort()\n",
    "# file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bd7e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_list = []\n",
    "for i in range(len(total_images_path)):\n",
    "    _pred_list.append(pred_list[i][:5])\n",
    "# _pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c381ef6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(_pred_list, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16868207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "535c5d41",
   "metadata": {},
   "source": [
    "with validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ced8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# import torch.nn.functional as F\n",
    "# import albumentations as A\n",
    "# import albumentations.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d060fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_valid_data(data_dir):\n",
    "#     img_valid_list = []\n",
    "#     label_valid_list = []\n",
    "    \n",
    "#     image_path = os.path.join(data_dir, 'dessert')\n",
    "    \n",
    "#     for product_name in os.listdir(image_path):\n",
    "#         product_path = os.path.join(image_path, product_name)\n",
    "#         if os.path.isdir(product_path):\n",
    "#             # get image path\n",
    "#             img_valid_list.extend(glob(os.path.join(product_path, '*.jpg')))\n",
    "#             img_valid_list.extend(glob(os.path.join(product_path, '*.png')))\n",
    "#             label = list(product_name[:5])\n",
    "            \n",
    "#             # get label\n",
    "#             label_valid_list.append(''.join(label))\n",
    "                \n",
    "#     return img_valid_list, label_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45d3d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valid_data_blanced(img, label):\n",
    "#     x = []\n",
    "#     y = []\n",
    "    \n",
    "#     for i in range(CFG['class']):\n",
    "#         _img = img[(i * 15): ((i + 1) * 15)]\n",
    "#         _label = label[i]\n",
    "        \n",
    "#         for img_product in _img:\n",
    "#             x.append(img_product)\n",
    "#             y.append(_label)\n",
    "            \n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "973a8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_valid_list, label_valid_list = get_valid_data('./Data/product_image/Validation/')\n",
    "# x_valid, y_valid = valid_data_blanced(img_valid_list, label_valid_list)\n",
    "# len(label_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b00b0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le2 = preprocessing.LabelEncoder()\n",
    "# targets_y = le2.fit_transform(y_valid)\n",
    "# targets_y = torch.as_tensor(targets_y)\n",
    "# one_hot_valid_y = F.one_hot(targets_y)\n",
    "# one_hot_valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e85ca4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlbumentationsCustomDataset(Dataset):\n",
    "#     def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "#         self.transforms = transforms\n",
    "#         self.train_mode = train_mode\n",
    "#         self.img_path_list = img_path_list\n",
    "#         self.label_list = label_list\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img_path = self.img_path_list[index]\n",
    "#         # Get image data\n",
    "#         image = cv2.imread(img_path)\n",
    "        \n",
    "#         # By default OpenCV uses BGR color space for color images,\n",
    "#         # so we need to convert the image to RGB color space.\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         if self.train_mode:\n",
    "# #             image = image.astype(np.int16)\n",
    "#             augmented = self.transforms(image=image)\n",
    "#             image = augmented['image']\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "#         else:\n",
    "#             image = self.transforms(image)\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8305a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_test_transform = albumentations.Compose([\n",
    "#                                     A.Resize(256, 256),\n",
    "#                                     A.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "# #                                     A.pytorch.transforms.ToTensor(),\n",
    "#                                     A.pytorch.transforms.ToTensorV2(transpose_mask=True),\n",
    "#                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccfea251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_vali_dataset = AlbumentationsCustomDataset(x_valid, one_hot_valid_y, train_mode=True, transforms=A_test_transform)\n",
    "# A_vali_loader = DataLoader(A_vali_dataset, batch_size = 5, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85729923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_for_acc(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     model_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for img, label in tqdm(iter(test_loader)):\n",
    "#             img = img.float().to(device)\n",
    "            \n",
    "#             pred_logit = model(img)\n",
    "#             pred_logit = pred_logit.squeeze().detach().cpu()\n",
    "            \n",
    "#             model_pred.extend(pred_logit.tolist())\n",
    "#     return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76b9b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7336bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if used_model == 'resnet':\n",
    "#     model = ResNet50().to(device)\n",
    "# elif used_model == 'efficientnet':\n",
    "#     model = EfficientNetb4().to(device)\n",
    "# elif used_model == 'regnet':\n",
    "#     model = RegNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8932b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "# preds = predict_for_acc(model, A_vali_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fdb7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = np.argmax(preds, axis=1)\n",
    "# true_labels = one_hot_valid_y.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f0b6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(true_labels, pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_16] *",
   "language": "python",
   "name": "conda-env-pytorch_16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
