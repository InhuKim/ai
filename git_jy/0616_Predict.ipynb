{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c3de41",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d30ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0c81cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e643018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':50,\n",
    "    'PATIENCE':10,\n",
    "    'class':14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb70e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_path = '/home/lab17/jupyter_home/Data/product_image/Training/'\n",
    "Valid_path = '/home/lab17/jupyter_home/Data/product_image/Validation/'\n",
    "Test_path = '/home/lab17/jupyter_home/git/test_img/'\n",
    "save_graph_path = './result/'\n",
    "save_model_path = '/home/lab17/jupyter_home/saved_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c340a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caabde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    image_path = os.path.join(data_dir, 'dessert')\n",
    "    \n",
    "    for product_name in os.listdir(image_path):\n",
    "        product_path = os.path.join(image_path, product_name)\n",
    "        if os.path.isdir(product_path):\n",
    "#             get image path\n",
    "#             img_path_list.extend(glob(os.path.join(product_path, '*.jpg')))\n",
    "#             img_path_list.extend(glob(os.path.join(product_path, '*.png')))\n",
    "            label = list(product_name[:5])\n",
    "            \n",
    "            # get label\n",
    "            label_list.append(''.join(label))\n",
    "                \n",
    "#     return img_path_list, label_list\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4b750e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = get_train_data(Train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7fc35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--targets\n",
      " [12 10  2  8  0  3 11  9  4  7 13  5  6  1]\n",
      "{'55701': 12, '45661': 10, '35211': 2, '45659': 8, '25222': 0, '35584': 3, '55034': 11, '45660': 9, '35585': 4, '45658': 7, '55702': 13, '45030': 5, '45657': 6, '25228': 1}\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(label_list)\n",
    "print('--targets\\n' , targets)\n",
    "\n",
    "label_encoder = {key:val for key, val in zip(label_list, targets)}\n",
    "print(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f87f431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    image_path = data_dir\n",
    "    \n",
    "#     for product in os.listdir(image_path):\n",
    "\n",
    "    # get image path\n",
    "    img_path_list.extend(glob(os.path.join(image_path, '*.jpg')))\n",
    "    img_path_list.extend(glob(os.path.join(image_path, '*.png')))\n",
    "    label_list = [ip[len('/home/lab17/jupyter_home/git/test_img/'):-6] for ip in img_path_list]\n",
    "\n",
    "    # get label\n",
    "#     label_list.append(''.join(label))\n",
    "                \n",
    "    return img_path_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15f3cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path, test_label_list = get_test_data(Test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6076d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_x = test_img_path\n",
    "# 레이블을 one-hot-vector로 변환\n",
    "test_y = [label_encoder[key] for key in test_label_list]\n",
    "test_targets = torch.as_tensor(test_y)\n",
    "one_hot_test_y = F.one_hot(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9280e427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['55034, 11', '45661, 10', '25228, 1', '25222, 0', '45659, 8', '55701, 12', '45030, 5', '35211, 2', '45660, 9', '45659, 8', '35585, 4', '55702, 13', '25222, 0', '55702, 13', '35211, 2', '45030, 5', '45661, 10', '35584, 3', '25222, 0', '55701, 12', '35211, 2', '35211, 2', '55701, 12', '35584, 3', '45660, 9', '55702, 13', '25222, 0', '45657, 6', '45657, 6', '55701, 12', '45657, 6']\n"
     ]
    }
   ],
   "source": [
    "print([f'{i}, {y}' for i, y in zip(test_label_list, test_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6b00271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsCustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        # Get image data\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.train_mode:\n",
    "#             image = image.astype(np.int16)\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "A_test_transform = albumentations.Compose([\n",
    "                                    A.Resize(256, 256),\n",
    "                                    A.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "#                                     A.pytorch.transforms.ToTensor(),\n",
    "                                    A.pytorch.transforms.ToTensorV2(transpose_mask=True),\n",
    "                                ])\n",
    "\n",
    "A_test_dataset = AlbumentationsCustomDataset(test_x, one_hot_test_y, train_mode=False, transforms=A_test_transform)\n",
    "A_test_loader = DataLoader(A_test_dataset, batch_size = 4, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5009cd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "300f3c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 2.61 GiB already allocated; 24.75 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m model_sch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCosineAnnealing\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#---------\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model_test \u001b[38;5;241m=\u001b[39m \u001b[43mResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# model_path = '/home/lab17/jupyter_home/saved_models/{}_{}_{}_{}_example.pth'.format(model_name, model_lr, model_optim, model_sch)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model_test.load_state_dict(torch.load(model_path))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# model_test.load_state_dict(torch.load('/home/lab17/jupyter_home/saved_models/{}_{}_{}_{}_example.pth'.format(model_name, model_lr, model_optim, model_sch))['state_dict'])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model_test\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/lab17/jupyter_home/saved_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_example.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name, model_lr, model_optim, model_sch)))\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_17/lib/python3.8/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_17/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_17/lib/python3.8/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_17/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 2.61 GiB already allocated; 24.75 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pred_ensemble = []\n",
    "batch_size = 34\n",
    "\n",
    "#---------\n",
    "model_name = 'ResNet50'\n",
    "model_lr = '0.0001'\n",
    "model_optim = 'adam'\n",
    "model_sch = 'CosineAnnealing' \n",
    "#---------\n",
    "\n",
    "model_test = ResNet50().to(device)\n",
    "# model_path = '/home/lab17/jupyter_home/saved_models/{}_{}_{}_{}_example.pth'.format(model_name, model_lr, model_optim, model_sch)\n",
    "# model_test.load_state_dict(torch.load(model_path))\n",
    "# model_test.load_state_dict(torch.load('/home/lab17/jupyter_home/saved_models/{}_{}_{}_{}_example.pth'.format(model_name, model_lr, model_optim, model_sch))['state_dict'])\n",
    "model_test.load_state_dict(torch.load('/home/lab17/jupyter_home/saved_models/{}_{}_{}_{}_example.pth'.format(model_name, model_lr, model_optim, model_sch)))\n",
    "model_test.eval()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loss = []\n",
    "\n",
    "for img, label in tqdm(iter(A_test_loader)):\n",
    "    img, label = img.float().to(device), label.float().to(device)\n",
    "    \n",
    "    # Data -> Model -> Output\n",
    "    logit = model_test(img)\n",
    "    logit = torch.squeeze(logit)\n",
    "    print(logit)\n",
    "    \n",
    "    # Calc loss\n",
    "    loss = criterion(logit, label)\n",
    "\n",
    "    test_loss.append(loss.item())\n",
    "    \n",
    "print(np.mean(test_loss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f_pred = []\n",
    "# pred_prob = []\n",
    "\n",
    "# image_data = Image.open('home/lab17/jupyter_home/Data/product_test/img.jpg')\n",
    "\n",
    "# image_transform = transforms.Compose([\n",
    "#     transforms.Resize(size=256),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.744859, 0.735139, 0.711357],\n",
    "#                          std=[0.100712, 0.120692, 0.167998])\n",
    "# ])\n",
    "\n",
    "# x = image_transform(image_data)\n",
    "# pred = model_test(x)\n",
    "# pred_prob.extend(pred.detach().cpu().numpy())\n",
    "# f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())\n",
    "\n",
    "# label_decoder = {val:key for key, val in zip(range(CFG['class']), sorted(label_list))}\n",
    "\n",
    "# f_result = [label_decoder[result] for result in f_pred]\n",
    "\n",
    "# print(f_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6275372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_17] *",
   "language": "python",
   "name": "conda-env-pytorch_17-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
