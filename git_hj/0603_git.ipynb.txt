{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c38e7b2",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81117f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  3 17:19:27 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   49C    P0    28W /  70W |   5356MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     14240      C   ...nvs/pytorch_16/bin/python     5353MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c90d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging  # 로그 출력\n",
    "import easydict  # 속성으로 dict 값에 access할 수 있음\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # process bar\n",
    "from os.path import join as opj\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c90e8a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff47ae",
   "metadata": {},
   "source": [
    "Hyper-parameter 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d4e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     # /home/lab18/Data/product_image/Training/image/10268_아넬라사과디저트_2입/\n",
    "     'data_path':'/home/lab16/jupyter_home/Data/product_image/',\n",
    "     'Kfold':1,\n",
    "     'model_path':'results/',\n",
    "\n",
    "     # Model parameter settings \n",
    "     'encoder_name':'regnety_064',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':224,\n",
    "     'batch_size':16,\n",
    "     'epochs':50,\n",
    "     'optimizer':'Lamb',\n",
    "#      'optimizer':'Adadelta',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-6,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':50,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':False,\n",
    "     'logging':False,\n",
    "     'num_workers':0,\n",
    "     'seed':42\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf0e95",
   "metadata": {},
   "source": [
    "# Utils for training and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74a4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimizer(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f420341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34eb9c5b",
   "metadata": {},
   "source": [
    "# Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e8c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29f8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f22fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ae0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 레이블을 one-hot-vector로 변환\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# targets = le.fit_transform(y)\n",
    "# targets = torch.as_tensor(targets)\n",
    "# one_hot_y = F.one_hot(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1892dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation용 추가\n",
    "class Valid_Dataset(Dataset):\n",
    "    def __init__(self, transform=None): \n",
    "        total_images_path = glob('/home/lab16/jupyter_home/Data/product_image/Validation/image/**/*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.file_name = file_names\n",
    "        \n",
    "        each_label = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            each_label.append(os.path.basename(total_images_path[i])[:5])\n",
    "            \n",
    "        # 레이블을 one-hot-vector로 변환\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(each_label)\n",
    "        targets = torch.as_tensor(targets)\n",
    "#         one_hot_y = F.one_hot(targets)\n",
    "            \n",
    "        self.target = np.array(targets) # 목표는 label\n",
    "#         self.target = one_hot_y\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Validation Dataset size:{len(self.file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx):  # train 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/train_256_new/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Validation/total_image/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "        # HWC => CHW-layout 변환\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ad6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "#     def __init__(self, df, transform=None):\n",
    "    def __init__(self, transform=None):\n",
    "#         self.file_name = df['file_name'].values      \n",
    "        \n",
    "        total_images_path = glob('/home/lab16/jupyter_home/Data/product_image/Training/image/**/*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.file_name = file_names\n",
    "                \n",
    "        each_label = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            each_label.append(os.path.basename(total_images_path[i])[:5])\n",
    "\n",
    "        # 레이블을 one-hot-vector로 변환\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(each_label)\n",
    "        targets = torch.as_tensor(targets)\n",
    "#         one_hot_y = F.one_hot(targets)\n",
    "            \n",
    "        self.target = np.array(targets) # 목표는 label\n",
    "#         self.target = one_hot_y\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Training Dataset size:{len(self.file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx):  # train 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/train_256_new/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Training/total_image/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "        # HWC => CHW-layout 변환\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.test_file_name = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.test_file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx): # test 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/test_256_new/', self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Test/total_image/', self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_file_name)\n",
    "\n",
    "# def get_loader(df, phase: str, batch_size, shuffle, num_workers, transform):\n",
    "def get_loader(phase: str, batch_size, shuffle, num_workers, transform):\n",
    "    if phase == 'test':\n",
    "#         dataset = Test_dataset(df, transform)  \n",
    "        dataset = Test_dataset(transform) \n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    # 임시로 추가\n",
    "    elif phase == 'validation':\n",
    "#         dataset = Train_Dataset(df, transform)\n",
    "        dataset = Valid_Dataset(transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    else:\n",
    "#         dataset = Train_Dataset(df, transform)\n",
    "        path = ''\n",
    "        dataset = Train_Dataset(transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "        \n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver == 1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "#                 transforms.ToTensor()\n",
    "                ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomPerspective(),\n",
    "                transforms.RandomAffine((20)),  # x, y축으로 이미지 늘림\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "#                 transforms.ToTensor()\n",
    "            ])\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ce401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "496a3605",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c73038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # 사전 학습된 모델 사용하기\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        \n",
    "        if 'regnet' in args.encoder_name:        \n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 76)\n",
    "        \n",
    "        elif 'efficient' in args.encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 76)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=0,\n",
    "                                    )\n",
    "        \n",
    "        if 'regnet' in encoder_name:        \n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 76)\n",
    "        \n",
    "        elif 'efficient' in encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 76)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574592ec",
   "metadata": {},
   "source": [
    "# Trainer for Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3456802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'device:{self.device}')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log_0603.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "#         df_train = pd.read_csv(opj(args.data_path, 'train_df.csv'))\n",
    "#         print('Read train_df.csv')\n",
    "\n",
    "#         kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "#         for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label'])):\n",
    "#             df_train.loc[val_idx, 'fold'] = fold\n",
    "#         val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "#         df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "#         df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "        \n",
    "        # 수정 - TrainLoader\n",
    "        self.train_loader = get_loader(phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(phase='validation', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        \n",
    "        # TrainLoader\n",
    "#         self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "#                                        num_workers=args.num_workers, transform=self.train_transform)\n",
    "#         self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "#                                        num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "#         self.optimizer = optim.Adadelta(self.model.parameters())\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "                \n",
    "            print(f'\\nbest epoch:{best_epoch}/loss:{best_loss:.4f}/f1:{best_f1:.4f}')\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66f503",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d2a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5763bf",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0beb201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_064', \n",
    "                             test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc682d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold X\n",
    "def result(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_064', \n",
    "                             test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = predict_list[0]\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8c70a",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93d602b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('./data/sample_submission.csv')\n",
    "# df_train = pd.read_csv('./data/train_df.csv')\n",
    "# df_test = pd.read_csv('./data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae8472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "# test_dataset = Test_dataset(df_test, test_transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe785ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e377aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e77ed23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 17:19:28,839 INFO: {'exp_num': '0', 'data_path': '/home/lab16/jupyter_home/Data/product_image/', 'Kfold': 1, 'model_path': 'results/', 'encoder_name': 'regnety_064', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 50, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 50, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 0, 'seed': 42, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "device:cuda\n",
      "Training Dataset size:8664\n",
      "Validation Dataset size:1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 17:19:29,392 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_064-0a48325c.pth)\n",
      "2022-06-03 17:19:33,056 INFO: Computational complexity:       6.37 GMac\n",
      "2022-06-03 17:19:33,057 INFO: Number of parameters:           29.38 M \n",
      "  3%|█                                         | 14/542 [00:57<35:52,  4.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m args\u001b[38;5;241m.\u001b[39mfold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m args\u001b[38;5;241m.\u001b[39mexp_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m models_path\u001b[38;5;241m.\u001b[39mappend(save_path)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create model directory\u001b[39;00m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_path\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, save_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Model weight in Multi_GPU or Single GPU\u001b[39;00m\n\u001b[1;32m     91\u001b[0m state_dict\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mstate_dict() \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmulti_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mTrainer.training\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    148\u001b[0m         scaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m    149\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), args\u001b[38;5;241m.\u001b[39mclipping)\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_16/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_16/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:285\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 285\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_16/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_16/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_16/lib/python3.8/site-packages/torch_optimizer/lamb.py:139\u001b[0m, in \u001b[0;36mLamb.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Apply bias to lr to avoid broadcast.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m bias_correction\n\u001b[0;32m--> 139\u001b[0m weight_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclamp_value)\n\u001b[1;32m    141\u001b[0m adam_step \u001b[38;5;241m=\u001b[39m exp_avg \u001b[38;5;241m/\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_16/lib/python3.8/site-packages/torch/functional.py:1543\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrobenius_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1545\u001b[0m     _dim \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim)]  \u001b[38;5;66;03m# noqa: C416 TODO: rewrite as list(range(m))\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fold 없이\n",
    "models_path = []\n",
    "args.fold = 0\n",
    "args.exp_num = str(0)\n",
    "save_path = main(args)\n",
    "models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold X\n",
    "result = result(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34beddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용한 모델의 batch_size, epoch, img_size, patience\n",
    "print('batch_size =', args.batch_size)\n",
    "print('epochs =', args.epochs)\n",
    "print('img_size =', args.img_size)\n",
    "print('patience =', args.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model =', args.encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ee487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_16]",
   "language": "python",
   "name": "conda-env-pytorch_16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
